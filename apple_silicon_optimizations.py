#!/usr/bin/env python3
"""
Apple Silicon M4 Pro ç‰¹åŒ–æœ€é©åŒ–
Audio Pipeline Integrated ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Š
"""
import torch
import platform
import multiprocessing
import psutil
import os

class AppleSiliconOptimizer:
    """Apple Silicon M4 Pro æœ€é©åŒ–ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.is_apple_silicon = self._detect_apple_silicon()
        self.memory_gb = self._get_memory_size()
        self.cpu_cores = multiprocessing.cpu_count()
        
    def _detect_apple_silicon(self):
        """Apple Siliconæ¤œå‡º"""
        if platform.system() != 'Darwin':
            return False
        
        try:
            import subprocess
            result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                  capture_output=True, text=True)
            cpu_info = result.stdout.strip()
            return 'Apple' in cpu_info
        except:
            return False
    
    def _get_memory_size(self):
        """ãƒ¡ãƒ¢ãƒªã‚µã‚¤ã‚ºå–å¾—ï¼ˆGBï¼‰"""
        try:
            memory_bytes = psutil.virtual_memory().total
            return memory_bytes / (1024**3)  # GBå¤‰æ›
        except:
            return 16  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    
    def get_optimal_device(self):
        """æœ€é©ãªãƒ‡ãƒã‚¤ã‚¹è¨­å®šã‚’å–å¾—"""
        if self.is_apple_silicon and torch.backends.mps.is_available():
            device = torch.device('mps')
            acceleration = "Apple Silicon GPU (MPS)"
        elif torch.cuda.is_available():
            device = torch.device('cuda')
            acceleration = "NVIDIA GPU (CUDA)"
        else:
            device = torch.device('cpu')
            acceleration = "CPU"
        
        return device, acceleration
    
    def get_optimal_batch_size(self, base_batch_size=4):
        """æœ€é©ãªãƒãƒƒãƒã‚µã‚¤ã‚ºè¨ˆç®—"""
        if self.is_apple_silicon:
            # M4 Pro + 48GBãƒ¡ãƒ¢ãƒªã§ã®æœ€é©åŒ–
            if self.memory_gb >= 32:
                multiplier = 4  # å¤§å¹…å¢—åŠ 
            elif self.memory_gb >= 16:
                multiplier = 2  # ä¸­ç¨‹åº¦å¢—åŠ 
            else:
                multiplier = 1  # æ¨™æº–
        else:
            multiplier = 1
        
        optimal_batch_size = base_batch_size * multiplier
        print(f"ğŸš€ æœ€é©ãƒãƒƒãƒã‚µã‚¤ã‚º: {optimal_batch_size} (base: {base_batch_size}, memory: {self.memory_gb:.1f}GB)")
        return optimal_batch_size
    
    def get_optimal_num_workers(self):
        """æœ€é©ãªãƒ¯ãƒ¼ã‚«ãƒ¼æ•°è¨ˆç®—"""
        if self.is_apple_silicon:
            # M4 Proã®åŠ¹ç‡ã‚³ã‚¢ãƒ»ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚³ã‚¢è€ƒæ…®
            # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ç”¨ã«æ§ãˆã‚ã«è¨­å®š
            optimal_workers = max(1, self.cpu_cores // 2)
        else:
            optimal_workers = max(1, self.cpu_cores // 4)
        
        print(f"ğŸ‘¥ æœ€é©ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°: {optimal_workers} (CPU cores: {self.cpu_cores})")
        return optimal_workers
    
    def setup_mps_optimizations(self):
        """MPSç‰¹åŒ–æœ€é©åŒ–è¨­å®š"""
        if not (self.is_apple_silicon and torch.backends.mps.is_available()):
            return False
        
        try:
            # MPSæœ€é©åŒ–ãƒ•ãƒ©ã‚°
            os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'  # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–
            
            print("ğŸ MPSæœ€é©åŒ–è¨­å®šã‚’é©ç”¨ã—ã¾ã—ãŸ")
            print(f"   â€¢ ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–: ON")
            print(f"   â€¢ åˆ©ç”¨å¯èƒ½ãƒ¡ãƒ¢ãƒª: {self.memory_gb:.1f}GB")
            return True
            
        except Exception as e:
            print(f"âš ï¸ MPSæœ€é©åŒ–è¨­å®šã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def get_audio_processing_config(self):
        """éŸ³å£°å‡¦ç†æœ€é©åŒ–è¨­å®š"""
        config = {
            'sample_rate': 48000,  # macOSæ¨™æº–
            'hop_length': 512,     # åŠ¹ç‡çš„ãªå‡¦ç†ã‚µã‚¤ã‚º
            'n_fft': 2048,        # é«˜å“è³ª
            'n_mels': 80,         # æ¨™æº–ãƒ¡ãƒ«æ•°
        }
        
        if self.is_apple_silicon:
            # Apple Siliconç‰¹åŒ–è¨­å®š
            config.update({
                'num_workers': self.get_optimal_num_workers(),
                'pin_memory': True,    # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡
                'persistent_workers': True,  # ãƒ¯ãƒ¼ã‚«ãƒ¼å†åˆ©ç”¨
            })
        
        return config
    
    def optimize_training_params(self, base_config):
        """å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–"""
        optimized = base_config.copy()
        
        if self.is_apple_silicon:
            # M4 Proæœ€é©åŒ–
            optimized.update({
                'batch_size': self.get_optimal_batch_size(base_config.get('batch_size', 4)),
                'num_workers': self.get_optimal_num_workers(),
                'learning_rate': base_config.get('learning_rate', 0.001) * 1.2,  # è‹¥å¹²é«˜é€ŸåŒ–
                'accumulation_steps': 1,  # å‹¾é…è“„ç©ãªã—ï¼ˆååˆ†ãªãƒ¡ãƒ¢ãƒªï¼‰
            })
            
            print("ğŸ¯ Apple Silicon M4 Pro å­¦ç¿’æœ€é©åŒ–:")
            for key, value in optimized.items():
                if key in ['batch_size', 'num_workers', 'learning_rate']:
                    print(f"   â€¢ {key}: {value}")
        
        return optimized
    
    def test_mps_performance(self):
        """MPSæ€§èƒ½ãƒ†ã‚¹ãƒˆ"""
        if not (self.is_apple_silicon and torch.backends.mps.is_available()):
            print("âŒ MPSåˆ©ç”¨ä¸å¯")
            return False
        
        try:
            print("ğŸ§ª MPSæ€§èƒ½ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
            
            device = torch.device('mps')
            
            # è¡Œåˆ—æ¼”ç®—ãƒ†ã‚¹ãƒˆ
            size = 2048
            a = torch.randn(size, size, device=device)
            b = torch.randn(size, size, device=device)
            
            import time
            start_time = time.time()
            for _ in range(10):
                c = torch.mm(a, b)
                torch.mps.synchronize()  # GPUåŒæœŸ
            end_time = time.time()
            
            avg_time = (end_time - start_time) / 10
            gflops = (2 * size ** 3) / (avg_time * 1e9)
            
            print(f"âœ… MPSæ€§èƒ½ãƒ†ã‚¹ãƒˆçµæœ:")
            print(f"   â€¢ è¡Œåˆ—ã‚µã‚¤ã‚º: {size}x{size}")
            print(f"   â€¢ å¹³å‡å‡¦ç†æ™‚é–“: {avg_time*1000:.2f}ms")
            print(f"   â€¢ æ€§èƒ½: {gflops:.1f} GFLOPS")
            
            return True
            
        except Exception as e:
            print(f"âŒ MPSæ€§èƒ½ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def create_optimized_dataloader(self, dataset, batch_size=None):
        """æœ€é©åŒ–ã•ã‚ŒãŸDataLoaderä½œæˆ"""
        if batch_size is None:
            batch_size = self.get_optimal_batch_size()
        
        dataloader_kwargs = {
            'batch_size': batch_size,
            'shuffle': True,
            'num_workers': self.get_optimal_num_workers(),
            'pin_memory': True,
            'drop_last': True,
        }
        
        if self.is_apple_silicon:
            # Apple Siliconç‰¹åŒ–è¨­å®š
            dataloader_kwargs['persistent_workers'] = True
        
        from torch.utils.data import DataLoader
        return DataLoader(dataset, **dataloader_kwargs)
    
    def print_optimization_summary(self):
        """æœ€é©åŒ–ã‚µãƒãƒªãƒ¼è¡¨ç¤º"""
        print("\nğŸ Apple Silicon M4 Pro æœ€é©åŒ–ã‚µãƒãƒªãƒ¼")
        print("=" * 50)
        print(f"ğŸ’» ã‚·ã‚¹ãƒ†ãƒ : {'Apple Silicon' if self.is_apple_silicon else 'Other'}")
        print(f"ğŸ§  CPU ã‚³ã‚¢æ•°: {self.cpu_cores}")
        print(f"ğŸ’¾ ãƒ¡ãƒ¢ãƒª: {self.memory_gb:.1f}GB")
        
        device, acceleration = self.get_optimal_device()
        print(f"ğŸš€ GPUåŠ é€Ÿ: {acceleration}")
        
        if self.is_apple_silicon:
            print(f"ğŸ“Š æœ€é©ãƒãƒƒãƒã‚µã‚¤ã‚º: {self.get_optimal_batch_size()}")
            print(f"ğŸ‘¥ æœ€é©ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°: {self.get_optimal_num_workers()}")
            print("ğŸ¯ macOS Core Audioçµ±åˆ: æº–å‚™å®Œäº†")
            print("ğŸµ 48kHzé«˜å“è³ªéŒ²éŸ³: å¯¾å¿œ")

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print("ğŸ Apple Silicon M4 Pro æœ€é©åŒ–ãƒ†ã‚¹ãƒˆ")
    print("=" * 50)
    
    optimizer = AppleSiliconOptimizer()
    optimizer.print_optimization_summary()
    
    # MPSè¨­å®š
    if optimizer.setup_mps_optimizations():
        optimizer.test_mps_performance()
    
    # éŸ³å£°å‡¦ç†è¨­å®š
    audio_config = optimizer.get_audio_processing_config()
    print(f"\nğŸµ éŸ³å£°å‡¦ç†è¨­å®š:")
    for key, value in audio_config.items():
        print(f"   â€¢ {key}: {value}")
    
    # å­¦ç¿’è¨­å®šä¾‹
    base_training_config = {
        'batch_size': 4,
        'learning_rate': 0.001,
        'epochs': 100
    }
    
    optimized_config = optimizer.optimize_training_params(base_training_config)
    print(f"\nğŸ¯ æœ€é©åŒ–å­¦ç¿’è¨­å®š:")
    for key, value in optimized_config.items():
        print(f"   â€¢ {key}: {value}")

if __name__ == "__main__":
    main()