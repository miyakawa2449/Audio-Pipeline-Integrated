"""
éŸ³å£°ã‚¯ãƒ­ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹
é«˜å“è³ªãªéŸ³å£°åˆæˆã¨ãƒ¡ãƒ«ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ æ­£è¦åŒ–æ©Ÿèƒ½ã‚’æä¾›
"""

import os
import shutil
import json
from datetime import datetime
from typing import List, Tuple, Optional

import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import torchaudio
import torchaudio.transforms as T

# ã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
try:
    import soundfile as sf
    SOUNDFILE_AVAILABLE = True
except ImportError:
    SOUNDFILE_AVAILABLE = False

try:
    from scipy import signal
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†…ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
from ..audio import AudioPreprocessor
from ..text import TextProcessor
from ..model import VoiceDataset, VoiceCloneModel

# çµ±ä¸€ãƒ­ã‚°ã‚·ã‚¹ãƒ†ãƒ ã¨å…±é€šãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
try:
    from common.logger import get_logger
    from common.audio_utils import get_audio_utils
    from common.device_utils import get_torch_device, get_device_utils
    from common.file_utils import setup_directories
except ImportError:
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®ãƒ€ãƒŸãƒ¼ãƒ­ã‚¬ãƒ¼
    import logging
    
    class FallbackLogger:
        def __init__(self, name):
            self.logger = logging.getLogger(name)
            self.logger.setLevel(logging.INFO)
            if not self.logger.handlers:
                handler = logging.StreamHandler()
                handler.setFormatter(logging.Formatter('%(levelname)s | %(message)s'))
                self.logger.addHandler(handler)
        
        def debug(self, msg): self.logger.debug(msg)
        def info(self, msg): self.logger.info(msg)
        def warning(self, msg): self.logger.warning(msg)
        def error(self, msg): self.logger.error(msg)
        def start_operation(self, msg): self.logger.info(f"ğŸš€ {msg} ã‚’é–‹å§‹")
        def complete_operation(self, msg): self.logger.info(f"âœ… {msg} ãŒå®Œäº†")
        def success(self, msg): self.logger.info(f"âœ… {msg}")
        def progress(self, msg): self.logger.info(f"ğŸ”„ {msg}")
        def audio_info(self, msg): self.logger.info(f"ğŸµ {msg}")
        def model_info(self, msg): self.logger.info(f"ğŸ¤– {msg}")
        def device_info(self, msg): self.logger.info(f"ğŸ›ï¸ {msg}")
    
    def get_logger(name): return FallbackLogger(name)
    def get_audio_utils(*args): return None
    def get_torch_device(): return torch.device('cpu')
    def get_device_utils(): return None
    def setup_directories(*args): return True


class VoiceCloner:
    """éŸ³å£°ã‚¯ãƒ­ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹"""
    
    # ã‚¯ãƒ©ã‚¹å®šæ•°
    DEFAULT_HOP_LENGTH = 256
    DEFAULT_N_FFT = 2048
    DEFAULT_N_MELS = 80
    MIN_FRAME_LENGTH = 50
    MEL_CLIP_MIN = -10
    MEL_CLIP_MAX = 10
    MEL_SCALE_MIN = -4
    MEL_SCALE_MAX = 4
    
    def __init__(self, dataset_path: str = "dataset"):
        """åˆæœŸåŒ–"""
        self.logger = get_logger("VoiceCloner")
        self.dataset_path = dataset_path
        self.audio_path = os.path.join(dataset_path, "audio_files")
        self.meta_path = os.path.join(dataset_path, "meta_files")
        self.processed_path = os.path.join(dataset_path, "processed")
        self.models_path = "models"
        self.output_path = "output"
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆï¼ˆå…±é€šãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ä½¿ç”¨ï¼‰
        directories = [self.audio_path, self.meta_path, self.processed_path, 
                      self.models_path, self.output_path]
        setup_directories(directories)
        
        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–
        self.preprocessor = AudioPreprocessor()
        self.text_processor = TextProcessor()
        self.model = None
        
        # å…±é€šéŸ³å£°ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
        self.audio_utils = get_audio_utils(self.preprocessor.sample_rate)
        
        # ãƒ‡ãƒã‚¤ã‚¹è¨­å®šï¼ˆå…±é€šãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ä½¿ç”¨ï¼‰
        self.device = get_torch_device()
        device_utils = get_device_utils()
        if device_utils:
            device_info = device_utils.get_compute_devices()
            if device_info['mps']['available']:
                self.logger.device_info("ãƒ‡ãƒã‚¤ã‚¹: Apple Silicon MPSåŠ é€Ÿ")
                self.logger.info("ğŸ Apple Silicon M4 Pro optimizations enabled")
            elif device_info['cuda']['available']:
                gpu_count = device_info['cuda']['device_count']
                self.logger.device_info(f"ãƒ‡ãƒã‚¤ã‚¹: CUDA GPUåŠ é€Ÿ ({gpu_count}ãƒ‡ãƒã‚¤ã‚¹)")
            else:
                self.logger.device_info("ãƒ‡ãƒã‚¤ã‚¹: CPUå‡¦ç†")
        else:
            self.logger.info(f"ãƒ‡ãƒã‚¤ã‚¹: {self.device}")
    
    # ==================== ãƒ‡ãƒ¼ã‚¿ç®¡ç† ====================
    
    def collect_data_files(self) -> Tuple[List[str], List[str]]:
        """ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åé›†ï¼ˆæŸ”è»Ÿãªæ¤œç´¢ï¼‰"""
        import glob
        import re
        
        audio_files = []
        text_files = []
        
        # audio_*.wav ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å…¨ã¦æ¤œç´¢
        audio_pattern = os.path.join(self.audio_path, "audio_*.wav")
        found_audio_files = glob.glob(audio_pattern)
        
        for audio_file in sorted(found_audio_files):
            # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ç•ªå·ã‚’æŠ½å‡º
            basename = os.path.basename(audio_file)
            match = re.match(r"audio_(\d+)\.wav", basename)
            if match:
                number = match.group(1)
                meta_file = os.path.join(self.meta_path, f"audio_{number}.txt")
                
                # å¯¾å¿œã™ã‚‹ãƒ¡ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿è¿½åŠ 
                if os.path.exists(meta_file):
                    audio_files.append(audio_file)
                    text_files.append(meta_file)
                else:
                    self.logger.warning(f"Missing meta file for {basename}")
        
        self.logger.info(f"Found {len(audio_files)} audio-text pairs")
        return audio_files, text_files
    
    def add_new_data(self, new_audio_path: str, new_text_path: str):
        """æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ """
        existing_files, _ = self.collect_data_files()
        next_index = len(existing_files) + 1
        
        new_audio_dest = os.path.join(self.audio_path, f"audio_{next_index}.wav")
        new_text_dest = os.path.join(self.meta_path, f"meta_{next_index}.txt")
        
        try:
            shutil.copy2(new_audio_path, new_audio_dest)
            shutil.copy2(new_text_path, new_text_dest)
            print(f"Added new data: audio_{next_index}.wav, meta_{next_index}.txt")
        except Exception as e:
            print(f"Error adding new data: {e}")
    
    # ==================== ãƒ¢ãƒ‡ãƒ«ç®¡ç† ====================
    
    def train_model(self, epochs: int = 50, batch_size: int = 4, learning_rate: float = 1e-3):
        """ãƒ¢ãƒ‡ãƒ«è¨“ç·´ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
        self.logger.start_operation("ãƒ¢ãƒ‡ãƒ«è¨“ç·´")
        
        try:
            # 1. ãƒ‡ãƒ¼ã‚¿æº–å‚™ãƒ•ã‚§ãƒ¼ã‚º
            dataloader = self._prepare_training_data(batch_size)
            if dataloader is None:
                return
            
            # 2. è¨“ç·´ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–
            optimizer, criterion = self._initialize_training_components(learning_rate)
            
            # 3. è¨“ç·´ãƒ«ãƒ¼ãƒ—å®Ÿè¡Œ
            self._execute_training_loop(epochs, dataloader, optimizer, criterion)
            
            self.logger.complete_operation("ãƒ¢ãƒ‡ãƒ«è¨“ç·´")
            
        except Exception as e:
            self.logger.error(f"ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def _prepare_training_data(self, batch_size: int) -> Optional[DataLoader]:
        """è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™"""
        # ãƒ‡ãƒ¼ã‚¿åé›†
        audio_files, text_files = self.collect_data_files()
        if len(audio_files) == 0:
            self.logger.error("No training data found!")
            return None
        
        # ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ã¨èªå½™æ§‹ç¯‰
        texts = self._load_and_process_texts(text_files)
        self.text_processor.build_vocab(texts)
        self.logger.info(f"Built vocabulary with {len(self.text_processor.vocab)} characters")
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
        dataset = VoiceDataset(audio_files, text_files, self.preprocessor, self.text_processor)
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, 
                               collate_fn=dataset.collate_fn)
        
        self.logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™å®Œäº†: {len(audio_files)} ãƒ•ã‚¡ã‚¤ãƒ«, {len(dataloader)} ãƒãƒƒãƒ")
        return dataloader
    
    def _load_and_process_texts(self, text_files: List[str]) -> List[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†"""
        texts = []
        for text_file in text_files:
            try:
                with open(text_file, 'r', encoding='utf-8') as f:
                    texts.append(self.text_processor.clean_text(f.read()))
            except Exception as e:
                self.logger.warning(f"ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ {text_file}: {e}")
                continue
        return texts
    
    def _initialize_training_components(self, learning_rate: float) -> Tuple[optim.Optimizer, nn.Module]:
        """è¨“ç·´ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®åˆæœŸåŒ–"""
        # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
        if self.model is None:
            vocab_size = len(self.text_processor.vocab)
            self.model = VoiceCloneModel(vocab_size).to(self.device)
            self.logger.info(f"ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å®Œäº†: èªå½™ã‚µã‚¤ã‚º {vocab_size}")
        
        # æœ€é©åŒ–å™¨ã¨æå¤±é–¢æ•°
        optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)
        criterion = nn.MSELoss()
        self.model.train()
        
        self.logger.info(f"æœ€é©åŒ–å™¨è¨­å®š: Adam, lr={learning_rate}")
        return optimizer, criterion
    
    def _execute_training_loop(self, epochs: int, dataloader: DataLoader, 
                              optimizer: optim.Optimizer, criterion: nn.Module):
        """è¨“ç·´ãƒ«ãƒ¼ãƒ—ã®å®Ÿè¡Œ"""
        for epoch in range(epochs):
            total_loss = 0
            num_batches = len(dataloader)
            
            for batch_idx, batch_data in enumerate(dataloader):
                loss = self._process_training_batch(batch_data, optimizer, criterion)
                total_loss += loss
            
            # é€²æ—ãƒ­ã‚°
            avg_loss = total_loss / num_batches
            self._log_training_progress(epoch + 1, epochs, avg_loss)
    
    def _process_training_batch(self, batch_data: Tuple, optimizer: optim.Optimizer, 
                               criterion: nn.Module) -> float:
        """å˜ä¸€ãƒãƒƒãƒã®è¨“ç·´å‡¦ç†"""
        audio_features, audio_lengths, text_sequences, text_lengths = batch_data
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•
        audio_features = audio_features.to(self.device)
        text_sequences = text_sequences.to(self.device)
        audio_lengths = audio_lengths.to(self.device)
        text_lengths = text_lengths.to(self.device)
        
        # å‹¾é…ãƒªã‚»ãƒƒãƒˆ
        optimizer.zero_grad()
        
        # é †ä¼æ’­
        mel_outputs, stop_tokens = self.model(text_sequences, text_lengths, audio_features)
        
        # æå¤±è¨ˆç®—ã¨é€†ä¼æ’­
        loss = criterion(mel_outputs, audio_features)
        loss.backward()
        optimizer.step()
        
        return loss.item()
    
    def _log_training_progress(self, epoch: int, total_epochs: int, avg_loss: float):
        """è¨“ç·´é€²æ—ã®ãƒ­ã‚°å‡ºåŠ›"""
        self.logger.debug(f"Epoch [{epoch}/{total_epochs}], Loss: {avg_loss:.4f}")
        
        # ä¸€å®šé–“éš”ã§è©³ç´°ãƒ­ã‚°
        if epoch % 10 == 0 or epoch == total_epochs:
            self.logger.info(f"è¨“ç·´é€²æ—: Epoch {epoch}/{total_epochs}, å¹³å‡æå¤±: {avg_loss:.4f}")
    
    def save_model(self, model_path: Optional[str] = None):
        """ãƒ¢ãƒ‡ãƒ«ä¿å­˜"""
        if model_path is None:
            model_path = os.path.join(self.models_path, "voice_clone_model.pth")
        
        if self.model is not None:
            torch.save({
                'model_state_dict': self.model.state_dict(),
                'text_processor': self.text_processor,
                'preprocessor': self.preprocessor
            }, model_path)
            print(f"Model saved to: {model_path}")
        else:
            print("No model to save!")
    
    def load_model(self, model_path: Optional[str] = None):
        """ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿"""
        if model_path is None:
            model_path = os.path.join(self.models_path, "voice_clone_model.pth")
        
        checkpoint = torch.load(model_path, map_location=self.device)
        
        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆå¾©å…ƒ
        self.text_processor = checkpoint['text_processor']
        self.preprocessor = checkpoint['preprocessor']
        
        # ãƒ¢ãƒ‡ãƒ«å†æ§‹ç¯‰
        vocab_size = len(self.text_processor.vocab)
        self.model = VoiceCloneModel(vocab_size=vocab_size).to(self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.eval()
        
        print(f"Model loaded from: {model_path}")
    
    # ==================== éŸ³å£°åˆæˆ ====================
    
    def synthesize_speech(self, text: str, output_filename: Optional[str] = None) -> bool:
        """éŸ³å£°åˆæˆã®ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
        if self.model is None:
            print("âŒ ãƒ¢ãƒ‡ãƒ«ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å…ˆã«ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ãã ã•ã„ã€‚")
            return False
        
        try:
            print(f"éŸ³å£°åˆæˆä¸­: '{text}'")
            
            # ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†
            text_tensor, text_lengths = self._preprocess_text(text)
            
            # ãƒ¡ãƒ«ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ ç”Ÿæˆ
            mel_outputs = self._generate_mel_spectrogram(text_tensor, text_lengths)
            
            # éŸ³å£°ç”Ÿæˆ
            audio = self._generate_audio(mel_outputs)
            
            # éŸ³å£°ä¿å­˜
            output_path = self._save_audio(audio, output_filename)
            
            # çµæœè¡¨ç¤º
            self._display_results(text, output_path, audio)
            self.logger.complete_operation(f"éŸ³å£°åˆæˆ: '{text}'")
            return True
            
        except Exception as e:
            self.logger.error(f"éŸ³å£°åˆæˆã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def _preprocess_text(self, text: str) -> Tuple[torch.Tensor, torch.Tensor]:
        """ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†"""
        text_sequence = self.text_processor.text_to_sequence(text)
        text_tensor = torch.LongTensor(text_sequence).unsqueeze(0).to(self.device)
        text_lengths = torch.LongTensor([len(text_sequence)]).to(self.device)
        
        self.logger.debug(f"ãƒ†ã‚­ã‚¹ãƒˆé•·: {len(text_sequence)} â†’ æœŸå¾…ã•ã‚Œã‚‹éŸ³å£°é•·: {len(text_sequence) * 0.1:.1f}ç§’")
        return text_tensor, text_lengths
    
    def _generate_mel_spectrogram(self, text_tensor: torch.Tensor, text_lengths: torch.Tensor) -> torch.Tensor:
        """ãƒ¡ãƒ«ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ ç”Ÿæˆ"""
        self.model.eval()
        with torch.no_grad():
            # ãƒ¢ãƒ‡ãƒ«æ¨è«–
            mel_outputs, stop_outputs = self.model(text_tensor, text_lengths)
            
            # æ­£è¦åŒ–
            mel_outputs = self._normalize_mel_spectrogram(mel_outputs)
            
            # é•·ã•åˆ¶å¾¡
            mel_outputs = self._control_length(mel_outputs, text_tensor)
        
        self.logger.debug(f"ç”Ÿæˆã•ã‚ŒãŸãƒ¡ãƒ«ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ :")
        self.logger.debug(f"  ãƒ•ãƒ¬ãƒ¼ãƒ æ•°: {mel_outputs.shape[1]}")
        self.logger.debug(f"  äºˆæƒ³éŸ³å£°é•·: {mel_outputs.shape[1] * self.DEFAULT_HOP_LENGTH / self.preprocessor.sample_rate:.2f}ç§’")
        self.logger.debug(f"  ãƒ¡ãƒ«ç¯„å›²: [{mel_outputs.min():.3f}, {mel_outputs.max():.3f}]")
        
        return mel_outputs.squeeze(0).cpu()
    
    def _normalize_mel_spectrogram(self, mel_outputs: torch.Tensor) -> torch.Tensor:
        """ãƒ¡ãƒ«ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ ã®æ­£è¦åŒ–"""
        # æ¥µç«¯ãªå€¤ã‚’ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
        mel_clipped = torch.clamp(mel_outputs, min=self.MEL_CLIP_MIN, max=self.MEL_CLIP_MAX)
        
        # æ­£è¦åŒ–
        mel_min, mel_max = mel_clipped.min(), mel_clipped.max()
        if mel_max > mel_min:
            mel_normalized = (mel_clipped - mel_min) / (mel_max - mel_min)
            mel_normalized = mel_normalized * (self.MEL_SCALE_MAX - self.MEL_SCALE_MIN) + self.MEL_SCALE_MIN
        else:
            mel_normalized = mel_clipped
        
        self.logger.debug(f"âœ“ ãƒ¡ãƒ«æ­£è¦åŒ–: [{mel_normalized.min():.3f}, {mel_normalized.max():.3f}]")
        
        # ç•°å¸¸å€¤ãƒã‚§ãƒƒã‚¯
        if torch.isnan(mel_normalized).any() or torch.isinf(mel_normalized).any():
            self.logger.warning("ãƒ¡ãƒ«ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ ã«ç•°å¸¸å€¤ã‚’æ¤œå‡ºã€‚ä¿®æ­£ä¸­...")
            mel_normalized = torch.nan_to_num(mel_normalized, nan=0.0, 
                                            posinf=self.MEL_SCALE_MAX, neginf=self.MEL_SCALE_MIN)
        
        return mel_normalized
    
    def _control_length(self, mel_outputs: torch.Tensor, text_tensor: torch.Tensor) -> torch.Tensor:
        """éŸ³å£°é•·åˆ¶å¾¡"""
        if mel_outputs.shape[1] < self.MIN_FRAME_LENGTH:
            self.logger.warning(f"å‡ºåŠ›ãŒçŸ­ã™ãã¾ã™ï¼ˆ{mel_outputs.shape[1]}ãƒ•ãƒ¬ãƒ¼ãƒ ï¼‰ã€‚æ‹¡å¼µä¸­...")
            
            target_length = max(self.MIN_FRAME_LENGTH, len(text_tensor[0]) * 15)
            last_frame = mel_outputs[:, -1:, :]
            repeat_count = target_length - mel_outputs.shape[1]
            
            # è‡ªç„¶ãªæ‹¡å¼µ
            decay_factor = torch.linspace(1.0, 0.8, repeat_count).unsqueeze(0).unsqueeze(2).to(mel_outputs.device)
            noise = torch.randn_like(last_frame.repeat(1, repeat_count, 1)) * 0.05
            padding = last_frame.repeat(1, repeat_count, 1) * decay_factor + noise
            
            mel_outputs = torch.cat([mel_outputs, padding], dim=1)
            self.logger.debug(f"âœ“ {target_length}ãƒ•ãƒ¬ãƒ¼ãƒ ã«æ‹¡å¼µã—ã¾ã—ãŸ")
        
        return mel_outputs
    
    def _generate_audio(self, mel_spec: torch.Tensor) -> torch.Tensor:
        """éŸ³å£°ç”Ÿæˆï¼ˆãƒœã‚³ãƒ¼ãƒ€ãƒ¼é¸æŠï¼‰"""
        self.logger.info("éŸ³å£°åˆæˆä¸­...")
        
        # äº”åéŸ³å°æœ¬ãƒ‡ãƒ¼ã‚¿ã®æœ‰ç„¡ã‚’ãƒã‚§ãƒƒã‚¯
        has_phoneme_data = self._check_phoneme_training_data()
        
        if has_phoneme_data:
            # å°æœ¬ãƒ‡ãƒ¼ã‚¿ã‚ã‚Šã®å ´åˆã®å„ªå…ˆé †ä½
            vocoder_methods = [
                (self._trained_phoneme_vocoder, "è¨“ç·´æ¸ˆã¿äº”åéŸ³ãƒœã‚³ãƒ¼ãƒ€ãƒ¼"),
                (self._japanese_phoneme_vocoder, "ç†è«–äº”åéŸ³ãƒœã‚³ãƒ¼ãƒ€ãƒ¼"),
                (self._reliable_vocoder, "ç¢ºå®Ÿã‚·ãƒ³ãƒ—ãƒ«ãƒœã‚³ãƒ¼ãƒ€ãƒ¼")
            ]
        else:
            # å°æœ¬ãƒ‡ãƒ¼ã‚¿ãªã—ã®å ´åˆã®å„ªå…ˆé †ä½ï¼ˆç¾åœ¨ã®çŠ¶æ…‹ï¼‰
            vocoder_methods = [
                (self._japanese_phoneme_vocoder, "äº”åéŸ³å¯¾å¿œãƒœã‚³ãƒ¼ãƒ€ãƒ¼"),
                (self._reliable_vocoder, "ç¢ºå®Ÿã‚·ãƒ³ãƒ—ãƒ«ãƒœã‚³ãƒ¼ãƒ€ãƒ¼"),
                (self._improved_vocoder, "æ”¹å–„ãƒœã‚³ãƒ¼ãƒ€ãƒ¼")
            ]
        
        for vocoder_func, vocoder_name in vocoder_methods:
            try:
                audio = vocoder_func(mel_spec)
                self.logger.info(f"ä½¿ç”¨ãƒœã‚³ãƒ¼ãƒ€ãƒ¼: {vocoder_name}")
                return self._postprocess_audio(audio)
            except Exception as e:
                self.logger.warning(f"{vocoder_name}ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        self.logger.error("å…¨ã¦ã®ãƒœã‚³ãƒ¼ãƒ€ãƒ¼ãŒå¤±æ•—ã€‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯éŸ³å£°ã‚’ç”Ÿæˆ...")
        duration = mel_spec.shape[0] * self.DEFAULT_HOP_LENGTH / self.preprocessor.sample_rate
        t = np.linspace(0, duration, int(duration * self.preprocessor.sample_rate))
        audio = 0.1 * np.sin(2 * np.pi * 200 * t)
        return torch.from_numpy(audio.astype(np.float32))
    
    def _postprocess_audio(self, audio: torch.Tensor) -> torch.Tensor:
        """éŸ³å£°å¾Œå‡¦ç†"""
        max_amp = torch.max(torch.abs(audio))
        self.logger.debug(f"ç”ŸæˆéŸ³å£°ã®æœ€å¤§æŒ¯å¹…: {max_amp:.6f}")
        
        if max_amp > 1.0:
            audio = audio / max_amp * 0.8
        elif max_amp < 0.01:
            audio = audio / max_amp * 0.5
        
        return audio
    
    def _save_audio(self, audio: torch.Tensor, output_filename: Optional[str] = None) -> str:
        """éŸ³å£°ä¿å­˜"""
        if output_filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_filename = f"synthesized_{timestamp}.wav"
        
        if not output_filename.endswith('.wav'):
            output_filename += '.wav'
        
        output_path = os.path.join(self.output_path, output_filename)
        torchaudio.save(output_path, audio.unsqueeze(0), self.preprocessor.sample_rate)
        
        return output_path
    
    def _display_results(self, text: str, output_path: str, audio: torch.Tensor):
        """çµæœè¡¨ç¤º"""
        duration = len(audio) / self.preprocessor.sample_rate
        # ã“ã‚Œã‚‰ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã¨ã—ã¦printã‚’ä¿æŒ
        print(f"âœ“ éŸ³å£°åˆæˆå®Œäº†!")
        print(f"  å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ: '{text}'")
        print(f"  å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {output_path}")
        print(f"  éŸ³å£°é•·: {duration:.2f}ç§’")
        print(f"  æœ€çµ‚æŒ¯å¹…ç¯„å›²: [{torch.min(audio):.3f}, {torch.max(audio):.3f}]")
        # ãƒ­ã‚°ã«ã‚‚è¨˜éŒ²
        self.logger.info(f"éŸ³å£°åˆæˆå®Œäº†: '{text}' â†’ {output_path} ({duration:.2f}ç§’)")
    
    # ==================== ãƒœã‚³ãƒ¼ãƒ€ãƒ¼å®Ÿè£… ====================
    
    def _reliable_vocoder(self, mel_spec: torch.Tensor) -> torch.Tensor:
        """ç¢ºå®Ÿã«å‹•ä½œã™ã‚‹ã‚·ãƒ³ãƒ—ãƒ«ãƒœã‚³ãƒ¼ãƒ€ãƒ¼"""
        self.logger.debug(f"ç¢ºå®Ÿãƒœã‚³ãƒ¼ãƒ€ãƒ¼å…¥åŠ›: {mel_spec.shape}")
        
        # ãƒ¡ãƒ«ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ ã®åŸºæœ¬ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        mel_np = mel_spec.T.numpy() if mel_spec.shape[1] == self.DEFAULT_N_MELS else mel_spec.numpy()
        n_frames, n_mels = mel_np.shape
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
        sample_rate = self.preprocessor.sample_rate
        hop_length = self.DEFAULT_HOP_LENGTH
        audio_length = n_frames * hop_length
        audio = np.zeros(audio_length)
        
        print(f"ãƒ•ãƒ¬ãƒ¼ãƒ æ•°: {n_frames}, äºˆæƒ³éŸ³å£°é•·: {audio_length / sample_rate:.2f}ç§’")
        
        # åŸºæœ¬å‘¨æ³¢æ•°è¨­å®šï¼ˆæ—¥æœ¬èªè©±è€…å‘ã‘ï¼‰
        f0_base = 150  # Hz
        
        # ãƒ•ãƒ¬ãƒ¼ãƒ ã”ã¨ã®éŸ³å£°ç”Ÿæˆ
        for frame_idx in range(n_frames):
            frame_start = frame_idx * hop_length
            frame_end = min(frame_start + hop_length, audio_length)
            frame_length = frame_end - frame_start
            
            frame_mel = mel_np[frame_idx, :]
            energy = np.mean(np.exp(frame_mel))
            
            if energy > 0.01:  # ã‚¨ãƒãƒ«ã‚®ãƒ¼é–¾å€¤
                t = np.arange(frame_length) / sample_rate
                frame_audio = np.zeros(frame_length)
                
                # åŸºæœ¬å‘¨æ³¢æ•°æ±ºå®š
                low_freq_energy = np.mean(frame_mel[:8])
                f0 = f0_base * (1 + low_freq_energy / 10)
                f0 = np.clip(f0, 80, 400)
                
                # èª¿æ³¢æ§‹é€ ã®ç”Ÿæˆ
                for harmonic in range(1, 4):  # 1-3æ¬¡èª¿æ³¢
                    freq = f0 * harmonic
                    if freq < sample_rate / 2:
                        mel_idx = min(int(harmonic * 8), n_mels - 1)
                        amplitude = np.exp(frame_mel[mel_idx] / 4) / harmonic
                        amplitude = np.clip(amplitude, 0, 1)
                        
                        sine_wave = amplitude * np.sin(2 * np.pi * freq * t)
                        frame_audio += sine_wave
                
                audio[frame_start:frame_end] = frame_audio
        
        # å¾Œå‡¦ç†
        if np.max(np.abs(audio)) > 0:
            audio = audio / np.max(np.abs(audio)) * 0.3
            
            # ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°
            if len(audio) > 3:
                for i in range(1, len(audio) - 1):
                    audio[i] = (audio[i-1] + audio[i] + audio[i+1]) / 3
        
        print(f"ç¢ºå®Ÿãƒœã‚³ãƒ¼ãƒ€ãƒ¼å®Œäº†: {len(audio)} samples, ç¯„å›²: [{audio.min():.3f}, {audio.max():.3f}]")
        return torch.from_numpy(audio.astype(np.float32))
    
    def _improved_vocoder(self, mel_spec: torch.Tensor) -> torch.Tensor:
        """æ”¹å–„ã•ã‚ŒãŸãƒœã‚³ãƒ¼ãƒ€ãƒ¼"""
        mel_np = mel_spec.T.numpy() if mel_spec.shape[1] == self.DEFAULT_N_MELS else mel_spec.numpy()
        
        sample_rate = self.preprocessor.sample_rate
        hop_length = self.DEFAULT_HOP_LENGTH
        audio_length = mel_np.shape[0] * hop_length
        audio = np.zeros(audio_length)
        
        # åŸºæœ¬å‘¨æ³¢æ•°æ¨å®š
        f0 = np.zeros(mel_np.shape[0])
        for i in range(mel_np.shape[0]):
            low_freq_power = mel_np[i, :30]
            if np.max(low_freq_power) > -35:
                f0[i] = 80 + np.argmax(low_freq_power) * 8
                
                if i > 0 and f0[i-1] > 0:
                    if abs(f0[i] - f0[i-1]) > 50:
                        f0[i] = (f0[i] + f0[i-1]) / 2
        
        # éŸ³å£°åˆæˆ
        for frame_idx in range(mel_np.shape[0]):
            frame_start = frame_idx * hop_length
            frame_end = min(frame_start + hop_length, audio_length)
            
            if f0[frame_idx] > 0:
                t = np.arange(frame_end - frame_start) / sample_rate
                
                for harmonic in range(1, 8):
                    freq = f0[frame_idx] * harmonic
                    if freq < sample_rate / 2:
                        freq_bin = min(int(freq * mel_np.shape[1] * 2 / sample_rate), mel_np.shape[1] - 1)
                        magnitude = mel_np[frame_idx, freq_bin]
                        
                        if magnitude > -45:
                            amplitude = np.exp(magnitude / 25) / (harmonic ** 0.7) * 0.15
                            phase = np.random.random() * 2 * np.pi
                            sine_wave = amplitude * np.sin(2 * np.pi * freq * t + phase)
                            audio[frame_start:frame_end] += sine_wave
        
        # æ­£è¦åŒ–ã¨ã‚¨ãƒ³ãƒ™ãƒ­ãƒ¼ãƒ—
        if np.max(np.abs(audio)) > 0:
            audio = audio / np.max(np.abs(audio)) * 0.7
            
            # ãƒ•ã‚§ãƒ¼ãƒ‰å‡¦ç†
            fade_samples = int(len(audio) * 0.02)
            if fade_samples > 0:
                audio[:fade_samples] *= np.linspace(0, 1, fade_samples)
                audio[-fade_samples:] *= np.linspace(1, 0, fade_samples)
        
        return torch.from_numpy(audio.astype(np.float32))
    
    def _simple_vocoder(self, mel_spec: torch.Tensor) -> torch.Tensor:
        """ã‚·ãƒ³ãƒ—ãƒ«ãƒœã‚³ãƒ¼ãƒ€ãƒ¼"""
        mel_np = mel_spec.numpy() if hasattr(mel_spec, 'numpy') else mel_spec.detach().cpu().numpy()
        
        sample_rate = self.preprocessor.sample_rate
        hop_length = self.DEFAULT_HOP_LENGTH
        audio_length = mel_np.shape[0] * hop_length
        audio = np.zeros(audio_length)
        
        # å‘¨æ³¢æ•°ãƒãƒƒãƒ”ãƒ³ã‚°
        mel_frequencies = np.linspace(0, sample_rate // 2, mel_np.shape[1])
        
        for frame_idx in range(mel_np.shape[0]):
            frame_start = frame_idx * hop_length
            frame_end = min(frame_start + hop_length, audio_length)
            
            for freq_bin in range(mel_np.shape[1]):
                magnitude = mel_np[frame_idx, freq_bin]
                frequency = mel_frequencies[freq_bin]
                
                if magnitude > -40:
                    t = np.arange(frame_end - frame_start) / sample_rate
                    amplitude = min(np.exp(magnitude / 20), 0.1)
                    phase = np.random.random() * 2 * np.pi
                    sine_wave = amplitude * np.sin(2 * np.pi * frequency * t + phase)
                    audio[frame_start:frame_end] += sine_wave
        
        # æ­£è¦åŒ–
        if np.max(np.abs(audio)) > 0:
            audio = audio / np.max(np.abs(audio)) * 0.8
        
        return torch.from_numpy(audio.astype(np.float32))

    def _japanese_phoneme_vocoder(self, mel_spec: torch.Tensor) -> torch.Tensor:
        """äº”åéŸ³è¡¨å¯¾å¿œæ—¥æœ¬èªéŸ³éŸ»ãƒœã‚³ãƒ¼ãƒ€ãƒ¼"""
        self.logger.debug(f"äº”åéŸ³å¯¾å¿œãƒœã‚³ãƒ¼ãƒ€ãƒ¼å…¥åŠ›: {mel_spec.shape}")
        
        # äº”åéŸ³ã®éŸ³éŸ»ç‰¹å¾´ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
        japanese_phonemes = {
            # ã‚è¡Œ (a-gyou)
            'a': {'formants': [730, 1090, 2440], 'f0_mod': 1.0, 'energy': 'high'},
            'i': {'formants': [270, 2290, 3010], 'f0_mod': 1.1, 'energy': 'medium'},
            'u': {'formants': [300, 870, 2240], 'f0_mod': 0.9, 'energy': 'low'},
            'e': {'formants': [530, 1840, 2480], 'f0_mod': 1.0, 'energy': 'medium'},
            'o': {'formants': [570, 840, 2410], 'f0_mod': 0.95, 'energy': 'medium'},
            
            # ã‹è¡Œ (ka-gyou) - å­éŸ³k + æ¯éŸ³
            'ka': {'formants': [730, 1200, 2400], 'f0_mod': 1.0, 'energy': 'high', 'burst': True},
            'ki': {'formants': [270, 2400, 3100], 'f0_mod': 1.1, 'energy': 'medium', 'burst': True},
            'ku': {'formants': [300, 900, 2200], 'f0_mod': 0.9, 'energy': 'low', 'burst': True},
            'ke': {'formants': [530, 1900, 2500], 'f0_mod': 1.0, 'energy': 'medium', 'burst': True},
            'ko': {'formants': [570, 900, 2400], 'f0_mod': 0.95, 'energy': 'medium', 'burst': True},
            
            # ã•è¡Œ (sa-gyou) - æ‘©æ“¦éŸ³
            'sa': {'formants': [730, 1200, 2600], 'f0_mod': 1.0, 'energy': 'high', 'fricative': True},
            'shi': {'formants': [300, 2200, 3200], 'f0_mod': 1.1, 'energy': 'medium', 'fricative': True},
            'su': {'formants': [300, 900, 2400], 'f0_mod': 0.9, 'energy': 'low', 'fricative': True},
            'se': {'formants': [530, 1900, 2600], 'f0_mod': 1.0, 'energy': 'medium', 'fricative': True},
            'so': {'formants': [570, 900, 2500], 'f0_mod': 0.95, 'energy': 'medium', 'fricative': True},
            
            # ãŸè¡Œ (ta-gyou) - ç ´è£‚éŸ³
            'ta': {'formants': [730, 1200, 2400], 'f0_mod': 1.0, 'energy': 'high', 'plosive': True},
            'chi': {'formants': [300, 2100, 3000], 'f0_mod': 1.1, 'energy': 'medium', 'plosive': True},
            'tsu': {'formants': [300, 900, 2300], 'f0_mod': 0.9, 'energy': 'low', 'plosive': True},
            'te': {'formants': [530, 1800, 2500], 'f0_mod': 1.0, 'energy': 'medium', 'plosive': True},
            'to': {'formants': [570, 900, 2400], 'f0_mod': 0.95, 'energy': 'medium', 'plosive': True},
            
            # ãªè¡Œ (na-gyou) - é¼»éŸ³
            'na': {'formants': [730, 1200, 2400], 'f0_mod': 1.0, 'energy': 'medium', 'nasal': True},
            'ni': {'formants': [270, 2200, 3000], 'f0_mod': 1.1, 'energy': 'medium', 'nasal': True},
            'nu': {'formants': [300, 900, 2200], 'f0_mod': 0.9, 'energy': 'low', 'nasal': True},
            'ne': {'formants': [530, 1800, 2500], 'f0_mod': 1.0, 'energy': 'medium', 'nasal': True},
            'no': {'formants': [570, 900, 2400], 'f0_mod': 0.95, 'energy': 'medium', 'nasal': True},
            
            # ã¯è¡Œ (ha-gyou) - æ‘©æ“¦éŸ³/åŠæ¯éŸ³
            'ha': {'formants': [730, 1200, 2400], 'f0_mod': 1.0, 'energy': 'medium', 'breath': True},
            'hi': {'formants': [270, 2200, 3100], 'f0_mod': 1.1, 'energy': 'medium', 'breath': True},
            'fu': {'formants': [300, 900, 2200], 'f0_mod': 0.9, 'energy': 'low', 'breath': True},
            'he': {'formants': [530, 1800, 2500], 'f0_mod': 1.0, 'energy': 'medium', 'breath': True},
            'ho': {'formants': [570, 900, 2400], 'f0_mod': 0.95, 'energy': 'medium', 'breath': True},
            
            # ã¾è¡Œ (ma-gyou) - é¼»éŸ³
            'ma': {'formants': [730, 1200, 2400], 'f0_mod': 1.0, 'energy': 'medium', 'nasal': True},
            'mi': {'formants': [270, 2200, 3000], 'f0_mod': 1.1, 'energy': 'medium', 'nasal': True},
            'mu': {'formants': [300, 900, 2200], 'f0_mod': 0.9, 'energy': 'low', 'nasal': True},
            'me': {'formants': [530, 1800, 2500], 'f0_mod': 1.0, 'energy': 'medium', 'nasal': True},
            'mo': {'formants': [570, 900, 2400], 'f0_mod': 0.95, 'energy': 'medium', 'nasal': True},
            
            # ã‚„è¡Œ (ya-gyou) - åŠæ¯éŸ³
            'ya': {'formants': [730, 1200, 2400], 'f0_mod': 1.0, 'energy': 'medium', 'glide': True},
            'yu': {'formants': [300, 900, 2200], 'f0_mod': 0.9, 'energy': 'low', 'glide': True},
            'yo': {'formants': [570, 900, 2400], 'f0_mod': 0.95, 'energy': 'medium', 'glide': True},
            
            # ã‚‰è¡Œ (ra-gyou) - æµéŸ³
            'ra': {'formants': [730, 1300, 2400], 'f0_mod': 1.0, 'energy': 'medium', 'liquid': True},
            'ri': {'formants': [270, 2300, 3000], 'f0_mod': 1.1, 'energy': 'medium', 'liquid': True},
            'ru': {'formants': [300, 1000, 2200], 'f0_mod': 0.9, 'energy': 'low', 'liquid': True},
            're': {'formants': [530, 1900, 2500], 'f0_mod': 1.0, 'energy': 'medium', 'liquid': True},
            'ro': {'formants': [570, 1000, 2400], 'f0_mod': 0.95, 'energy': 'medium', 'liquid': True},
            
            # ã‚è¡Œ (wa-gyou) - åŠæ¯éŸ³
            'wa': {'formants': [730, 1200, 2400], 'f0_mod': 1.0, 'energy': 'medium', 'glide': True},
            'wo': {'formants': [570, 900, 2400], 'f0_mod': 0.95, 'energy': 'medium', 'glide': True},
            
            # ã‚“ (n) - é¼»éŸ³
            'n': {'formants': [400, 1200, 2400], 'f0_mod': 0.8, 'energy': 'low', 'nasal': True}
        }
        
        # ãƒ¡ãƒ«ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ ã®åŸºæœ¬ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        mel_np = mel_spec.T.numpy() if mel_spec.shape[1] == self.DEFAULT_N_MELS else mel_spec.numpy()
        n_frames, n_mels = mel_np.shape
        
        sample_rate = self.preprocessor.sample_rate
        hop_length = self.DEFAULT_HOP_LENGTH
        audio_length = n_frames * hop_length
        audio = np.zeros(audio_length)
        
        print(f"ãƒ•ãƒ¬ãƒ¼ãƒ æ•°: {n_frames}, äºˆæƒ³éŸ³å£°é•·: {audio_length / sample_rate:.2f}ç§’")
        
        # ãƒ•ãƒ¬ãƒ¼ãƒ ã”ã¨ã®å‡¦ç†
        for frame_idx in range(n_frames):
            frame_start = frame_idx * hop_length
            frame_end = min(frame_start + hop_length, audio_length)
            frame_length = frame_end - frame_start
            
            frame_mel = mel_np[frame_idx, :]
            
            # ã‚¨ãƒãƒ«ã‚®ãƒ¼æ¤œå‡º
            energy = np.mean(np.exp(frame_mel))
            if energy > 0.005:
                t = np.arange(frame_length) / sample_rate
                frame_audio = np.zeros(frame_length)
                
                # éŸ³éŸ»ã‚¿ã‚¤ãƒ—ã®æ¨å®šï¼ˆã‚ˆã‚Šè©³ç´°ï¼‰
                phoneme_type = self._estimate_phoneme_type(frame_mel)
                phoneme_data = japanese_phonemes.get(phoneme_type, japanese_phonemes['a'])
                
                # åŸºæœ¬å‘¨æ³¢æ•°ã®æ±ºå®š
                low_freq_energy = np.mean(frame_mel[:12])
                f0_base = 140 * phoneme_data['f0_mod']  # éŸ³éŸ»ã«å¿œã˜ãŸåŸºæœ¬å‘¨æ³¢æ•°
                f0 = f0_base * (1 + low_freq_energy / 8)
                f0 = np.clip(f0, 70, 350)
                
                # ãƒ•ã‚©ãƒ«ãƒãƒ³ãƒˆåˆæˆ
                frame_audio += self._synthesize_formants(
                    t, phoneme_data['formants'], frame_mel, n_mels, sample_rate
                )
                
                # éŸ³éŸ»ç‰¹æ€§ã«å¿œã˜ãŸè¿½åŠ å‡¦ç†
                if phoneme_data.get('burst'):
                    frame_audio += self._add_burst_noise(t, frame_mel)
                elif phoneme_data.get('fricative'):
                    frame_audio += self._add_fricative_noise(t, frame_mel)
                elif phoneme_data.get('nasal'):
                    frame_audio += self._add_nasal_resonance(t, f0, frame_mel)
                elif phoneme_data.get('breath'):
                    frame_audio += self._add_breath_noise(t, frame_mel)
                
                # åŸºæœ¬å‘¨æ³¢æ•°æˆåˆ†ã®è¿½åŠ 
                frame_audio += self._add_fundamental_harmonics(t, f0, frame_mel, n_mels)
                
                audio[frame_start:frame_end] = frame_audio
        
        # å¾Œå‡¦ç†
        audio = self._postprocess_japanese_audio(audio)
        
        print(f"äº”åéŸ³å¯¾å¿œãƒœã‚³ãƒ¼ãƒ€ãƒ¼å®Œäº†: {len(audio)} samples, ç¯„å›²: [{audio.min():.3f}, {audio.max():.3f}]")
        return torch.from_numpy(audio.astype(np.float32))

    def _estimate_phoneme_type(self, frame_mel):
        """ãƒ¡ãƒ«ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ ã‹ã‚‰éŸ³éŸ»ã‚¿ã‚¤ãƒ—ã‚’æ¨å®š"""
        low_energy = np.mean(frame_mel[:15])      # ä½åŸŸ (0-1500Hz)
        mid_energy = np.mean(frame_mel[15:40])    # ä¸­åŸŸ (1500-4000Hz)  
        high_energy = np.mean(frame_mel[40:])     # é«˜åŸŸ (4000Hz+)
        
        # ã‚¨ãƒãƒ«ã‚®ãƒ¼åˆ†å¸ƒã«åŸºã¥ãéŸ³éŸ»æ¨å®š
        if high_energy > mid_energy and high_energy > low_energy:
            if mid_energy > low_energy:
                return 'i'  # ã„ç³»
            else:
                return 'shi'  # ã—ç³»ï¼ˆæ‘©æ“¦éŸ³ï¼‰
        elif low_energy > mid_energy and low_energy > high_energy:
            if mid_energy < -30:
                return 'u'  # ã†ç³»
            else:
                return 'o'  # ãŠç³»
        elif mid_energy > low_energy and mid_energy > high_energy:
            return 'e'  # ãˆç³»
        else:
            return 'a'  # ã‚ç³»ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰

    def _synthesize_formants(self, t, formants, frame_mel, n_mels, sample_rate):
        """ãƒ•ã‚©ãƒ«ãƒãƒ³ãƒˆåˆæˆ"""
        audio = np.zeros(len(t))
        
        for formant_freq in formants:
            if formant_freq < sample_rate / 2:
                # ãƒ•ã‚©ãƒ«ãƒãƒ³ãƒˆå‘¨æ³¢æ•°ã«å¯¾å¿œã™ã‚‹ãƒ¡ãƒ«bin
                mel_bin = min(int(formant_freq * n_mels / (sample_rate/2)), n_mels-1)
                formant_energy = frame_mel[mel_bin]
                
                if formant_energy > -35:
                    # ãƒ•ã‚©ãƒ«ãƒãƒ³ãƒˆã®æŒ¯å¹…
                    amplitude = np.exp(formant_energy / 12) * 0.08
                    amplitude = np.clip(amplitude, 0, 0.2)
                    
                    # ãƒ•ã‚©ãƒ«ãƒãƒ³ãƒˆå¸¯åŸŸå¹…
                    bandwidth = formant_freq * 0.08
                    
                    # å¸¯åŸŸå¹…ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
                    for offset in [-bandwidth/2, 0, bandwidth/2]:
                        freq = formant_freq + offset
                        if freq > 0 and freq < sample_rate / 2:
                            sine_wave = amplitude * np.sin(2 * np.pi * freq * t)
                            audio += sine_wave / 3
        
        return audio

    def _add_burst_noise(self, t, frame_mel):
        """ç ´è£‚éŸ³ã®ãƒãƒ¼ã‚¹ãƒˆé›‘éŸ³"""
        if np.mean(frame_mel[20:40]) > -30:  # ä¸­é«˜åŸŸã«ã‚¨ãƒãƒ«ã‚®ãƒ¼ãŒã‚ã‚‹å ´åˆ
            burst_duration = min(len(t), int(len(t) * 0.1))  # 10%ã®é•·ã•
            burst_noise = np.random.random(burst_duration) * 0.05
            audio = np.zeros(len(t))
            audio[:burst_duration] = burst_noise
            return audio
        return np.zeros(len(t))

    def _add_fricative_noise(self, t, frame_mel):
        """æ‘©æ“¦éŸ³ã®é›‘éŸ³"""
        if np.mean(frame_mel[30:]) > -35:  # é«˜åŸŸã«ã‚¨ãƒãƒ«ã‚®ãƒ¼ãŒã‚ã‚‹å ´åˆ
            # é«˜å‘¨æ³¢é›‘éŸ³
            noise = np.random.random(len(t)) * 0.03
            # é«˜åŸŸãƒ•ã‚£ãƒ«ã‚¿
            cutoff_freq = 2000  # 2kHzä»¥ä¸Š
            noise_filtered = noise  # ç°¡æ˜“å®Ÿè£…
            return noise_filtered
        return np.zeros(len(t))

    def _add_nasal_resonance(self, t, f0, frame_mel):
        """é¼»éŸ³ã®å…±é³´"""
        if f0 > 0:
            # é¼»è…”å…±é³´å‘¨æ³¢æ•° (ç´„1000Hz)
            nasal_freq = 1000
            amplitude = np.exp(np.mean(frame_mel[:20]) / 15) * 0.05
            amplitude = np.clip(amplitude, 0, 0.1)
            
            nasal_tone = amplitude * np.sin(2 * np.pi * nasal_freq * t)
            return nasal_tone
        return np.zeros(len(t))

    def _add_breath_noise(self, t, frame_mel):
        """æ°—æ¯éŸ³ã®è¿½åŠ """
        if np.mean(frame_mel) > -30:
            # ä½ãƒ¬ãƒ™ãƒ«ã®åºƒå¸¯åŸŸé›‘éŸ³
            breath = np.random.random(len(t)) * 0.02
            return breath
        return np.zeros(len(t))

    def _add_fundamental_harmonics(self, t, f0, frame_mel, n_mels):
        """åŸºæœ¬å‘¨æ³¢æ•°ã¨èª¿æ³¢ã®è¿½åŠ """
        audio = np.zeros(len(t))
        
        if f0 > 0:
            for harmonic in range(1, 6):  # 1-5æ¬¡èª¿æ³¢
                freq = f0 * harmonic
                if freq < 11000:  # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å‘¨æ³¢æ•°ã®åŠåˆ†æœªæº€
                    mel_idx = min(int(harmonic * 10), n_mels - 1)
                    amplitude = np.exp(frame_mel[mel_idx] / 15) / (harmonic ** 0.7)
                    amplitude = np.clip(amplitude, 0, 0.15)
                    
                    sine_wave = amplitude * np.sin(2 * np.pi * freq * t)
                    audio += sine_wave
        
        return audio

    def _postprocess_japanese_audio(self, audio):
        """æ—¥æœ¬èªéŸ³å£°ç‰¹åŒ–ã®å¾Œå‡¦ç†"""
        if np.max(np.abs(audio)) > 0:
            # æ­£è¦åŒ–
            audio = audio / np.max(np.abs(audio)) * 0.35
            
            # ã‚ˆã‚Šè‡ªç„¶ãªã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°
            if len(audio) > 7:
                window_size = 7
                for i in range(window_size//2, len(audio) - window_size//2):
                    audio[i] = np.mean(audio[i-window_size//2:i+window_size//2+1])
            
            # ãƒ•ã‚§ãƒ¼ãƒ‰å‡¦ç†
            fade_samples = int(len(audio) * 0.005)  # 0.5%
            if fade_samples > 0:
                audio[:fade_samples] *= np.linspace(0, 1, fade_samples)
                audio[-fade_samples:] *= np.linspace(1, 0, fade_samples)
        
        return audio

    def _check_phoneme_training_data(self) -> bool:
        """äº”åéŸ³å°æœ¬ãƒ‡ãƒ¼ã‚¿ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯"""
        phoneme_data_path = os.path.join(self.dataset_path, "phoneme_data")
        return os.path.exists(phoneme_data_path) and len(os.listdir(phoneme_data_path)) > 0

    def _trained_phoneme_vocoder(self, mel_spec: torch.Tensor) -> torch.Tensor:
        """è¨“ç·´æ¸ˆã¿äº”åéŸ³ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ãŸãƒœã‚³ãƒ¼ãƒ€ãƒ¼"""
        print("å®Ÿæ¸¬ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãäº”åéŸ³ãƒœã‚³ãƒ¼ãƒ€ãƒ¼ã‚’ä½¿ç”¨")
        
        # å®Ÿéš›ã®éŒ²éŸ³ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æŠ½å‡ºã—ãŸãƒ•ã‚©ãƒ«ãƒãƒ³ãƒˆç‰¹å¾´ã‚’ä½¿ç”¨
        measured_phonemes = self._load_measured_phoneme_features()
        
        # æ—¢å­˜ã®_japanese_phoneme_vocoderã‚’ãƒ™ãƒ¼ã‚¹ã«
        # measured_phonemesã®å®Ÿæ¸¬å€¤ã§ä¸Šæ›¸ã
        return self._synthesize_with_measured_features(mel_spec, measured_phonemes)